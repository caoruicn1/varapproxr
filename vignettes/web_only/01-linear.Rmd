---
title: "Linear Models"
author: "James Totterdell"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: show
    fig_retina: null
bibliography: ../references.bib
vignette: >
  %\VignetteIndexEntry{linear}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(varapproxr)
```

# Normal Linear Regression

We consider the following model
$$
\begin{aligned}
p(y|\beta,\sigma^2) &= \text{Normal}(y|X\beta, \Sigma) \\
p(\beta) &= \text{Normal}(\beta|\mu_0, \Sigma_0) \\
\Sigma &= \sigma^2 I_N
\end{aligned}
$$
with either $p(\sigma^2) = \text{Inverse-Gamma}(\sigma^2|a_0, b_0)$ or $p(\sigma) = \text{Half-}t(\sigma^2|a_0,b_0)$. 
The latter can be expressed as
$$
\begin{aligned}
p(\sigma^2|\lambda) &= \text{Inverse-Gamma}(b_0/2, b_0/\lambda) \\
p(\lambda) &= \text{Inverse-Gamma}(1/2,1/a_0^2) \\
\implies p(\sigma) &= \text{Half-}t(a_0,b_0)
\end{aligned}
$$

## Inverse-Gamma prior on $\sigma^2$

The joint posterior density $p(\beta,\sigma^2,\lambda|y)$ is approximated by
$$
q(\beta,\sigma^2,\lambda) = q(\beta)q(\sigma^2)
$$
with optimal solutions
$$
\begin{aligned}
q^\star(\beta) \propto \exp\left\{\mathbb E_{\sigma^2}\left[\ln p(\beta|y,\sigma^2)\right]\right\} \\
q^\star(\sigma^2) \propto \exp\left\{\mathbb E_{\beta}\left[\ln p(\sigma^2|y,\beta)\right]\right\}.
\end{aligned}
$$

From the model above, $\ln p(y,\beta,\sigma^2) = \ln p(y|\beta,\sigma^2) + \ln p(\beta) + \ln p(\sigma^2|\lambda) + \ln p(\lambda)$.
For the regression coefficients, the full conditional is
$$
\begin{aligned}
\ln p(\beta | y, \sigma^2) &= \text{const} - \frac{1}{2}\left[(y-X\beta)^\top\Sigma^{-1}(y-X\beta) - \frac{1}{2}(\beta-\mu_0)^\top\Sigma_0^{-1}(\beta-\mu_0)\right] \\
&= \text{const} - \frac{1}{2}\left[\left(\beta - M^{-1}m\right)^\top M\left(\beta - M^{-1}m\right)\right]\\
M &= X^\top\Sigma^{-1}X + \Sigma_0^{-1} \\
m &= X^\top\Sigma^{-1}y + \Sigma_0^{-1}\mu_0 \\
\implies p(\beta|y,\sigma^2) &= \text{Normal}(\mu_{\beta|y,\sigma^2}, \Sigma_{\beta|y,\sigma^2}) \\
\Sigma_{\beta|y,\sigma^2} &= \left(X^\top\Sigma^{-1}X + \Sigma_0^{-1}\right)^{-1} \\
\mu_{\beta|y,\sigma^2} &= \Sigma_{\beta|y,\sigma^2}\left(X^\top\Sigma^{-1}y + \Sigma_0^{-1}\mu_0\right).
\end{aligned}
$$
From which, the optimal density is
$$
\begin{aligned}
\mathbb E_{q(\sigma^2)}[\ln p(\beta|y,\sigma^2)] &= \text{const} - \frac{1}{2}\left[\left(\beta - M_q^{-1}m_q\right)^\top M_q\left(\beta - M_q^{-1}m_q\right)\right] \\
M_q &= \mathbb E_{q(\sigma^2)}\left[\sigma^{-2}\right]X^\top X + \Sigma_0^{-1} \\
m_q &= \mathbb E_{q(\sigma^2)}\left[\sigma^{-2}\right] X^\top y + \Sigma_0^{-1}\mu_0 \\
\implies q^\star(\beta) &= \text{Normal}(\mu_{q(\beta)}, \Sigma_{q(\beta)}) \\
\Sigma_{q(\beta)} &= \left(\mathbb E_{q(\sigma^2)}\left[\sigma^{-2}\right] X^\top X + \Sigma_0^{-1}\right)^{-1} \\
\mu_{q(\beta)} &= \Sigma_{q(\beta)}\left(\mathbb E_{q(\sigma^2)}\left[\sigma^{-2}\right] X^\top y + \Sigma_0^{-1}\mu_0\right).
\end{aligned}
$$

For the variance component,
$$
\begin{aligned}
\ln p(\sigma^2|y,\beta) &= \text{const} + \frac{P}{2}\ln(\sigma^{2})-\frac{1}{2}\sigma^{-2}\left[(y-X\beta)^\top(y-X\beta)\right] - (a_0+1)\ln(\sigma^2) - \sigma^{-2}b_0\\
\implies p(\sigma^2|y,\beta) &= \text{Inverse-Gamma}(\sigma^2|a_{\sigma^2|\beta,y}, b_{\sigma^2|\beta,y}) \\
a_{\sigma^2|\beta,y} &= a_0 + \frac{N}{2} \\
b_{\sigma^2|\beta,y} &= b_0 + \frac{\lVert y-X\beta\rVert^2}{2}
\end{aligned}
$$
From which, the optimal density is
$$
\begin{aligned}
q^\star(\sigma^2) &\propto \mathbb E_{q(\beta)}\left[\ln p(\sigma^2|y,\beta)\right] \\
\implies q^\star(\sigma^2) &= \text{Inverse-Gamma}(a_{q(\sigma^2)}, b_{q(\sigma^2)}) \\
a_{q(\sigma^2)} &= a_0 + \frac{N}{2} \\
b_{q(\sigma^2)} &= b_0 + \frac{\lVert y - X\mu_{q(\beta)}\rVert^2+\text{tr}(X^\top X\Sigma_{q(\beta)})}{2}
\end{aligned}
$$
implying that
$$
\mathbb E_{q(\sigma^2)}\left[\sigma^{-2}\right] = \frac{a_{q(\sigma^2)}}{b_{q(\sigma^2)}}
$$
in the variational parameters for $q^\star(\beta)$.

The lower bound itself is given by
$$
\begin{aligned}
\mathcal{L}(q) &= \mathbb E_q[\ln p(y,\beta,\sigma^2) - q(\beta,\sigma^2)] \\ 
&= \mathbb E_q[\ln p(y|\beta,\sigma^2)] + \mathbb E_q[\ln p(\beta)] + \mathbb E_q[\ln p(\sigma^2)] + 
\mathbb H_q[\beta] + \mathbb H_q[\sigma^2] \\
&= 
\end{aligned}
$$

The updates are then
$$
\begin{aligned}
a_{q(\sigma^2)} &\leftarrow a_0 + N/2 \\
\text{Cycle:} \\
  \Sigma_{q(\beta)} &\leftarrow  \left(\frac{a_{q(\sigma^2)}}{b_{q(\sigma^2)}}X^\top X + \Sigma_0^{-1}\right)^{-1} \\
  \mu_{q(\beta)} &\leftarrow \Sigma_{q(\beta)}\left(\frac{a_{q(\sigma^2)}}{b_{q(\sigma^2)}} X^\top y + \Sigma_0^{-1}\mu_0\right) \\
  b_{q(\sigma^2)} &\leftarrow b_0 + \frac{\lVert y - X\mu_{q(\beta)}\rVert^2+\text{tr}(X^\top X\Sigma_{q(\beta)})}{2}
\end{aligned}
$$
until the change in $\mathcal{L}(q)$ is below a specified tolerance level indicating convergence.


## Half-$t$ prior on $\sigma$

The optimal density for $\beta$ is unchanged from the previous section.


# $t$ Linear Regression

[@wand2010]

We replace $p(y|\beta,\sigma^2) = \text{Normal}(y|X\beta, \Sigma)$ by
$$
\begin{aligned}
p(y_i|\beta,\sigma,\nu) &= \text{Student-t}(x_i^\top\beta, \sigma, \nu) \\
p(\nu) &= \text{Uniform}(\nu_0, \nu_1)
\end{aligned}
$$
which is equivalent to
$$
\begin{aligned}
p(y_i|\beta,\sigma,\nu) &= \text{Normal}(x_i^\top\beta, \lambda_i\sigma^2) \\
p(\lambda_i|\nu) &= \text{Inverse-Gamma}(\nu/2, \nu/2) \\
p(\nu) &= \text{Uniform}(\nu_0, \nu_1).
\end{aligned}
$$

Assuming an inverse-gamma prior on $\sigma^2$, the full-conditionals satisfy
$$
\begin{aligned}
\ln p(\beta|y,\sigma^2,\nu,\lambda) &= \text{const} \\
\ln p(\sigma^2|y,\beta,\nu,\lambda) &= \text{const} -(a_0+n/2)\ln(\sigma^2) - \left[b_0+\frac{1}{2}\left((y-X\beta)^\top \text{diag}(1/\lambda)(y - X\beta)\right)\right]/\sigma^2 \\
\ln p(\lambda_i|y,\beta,\nu,\sigma^2) &= \text{const} + \sum_{i=1}^n\left[ -\frac{1}{2}(\nu + 1)\ln(\lambda_i) - \frac{1}{2}\left[\nu + \sigma^{-2}(y_i-x_i^\top\beta)^2\right]/\lambda_i\right] \\
\ln p(\nu|y,\beta,\sigma^2,\lambda) &= \text{const} + n\left[\frac{\nu}{2}\ln(\nu/2)-\ln\Gamma(\nu/2)\right]-(\nu/2)1^\top(\ln\lambda+1/\lambda),\quad\nu_0<\nu<\nu_1
\end{aligned}
$$

We use the factorisation $q(\beta,\nu,\sigma^2,\lambda)=q(\beta,\nu)q(\sigma^2)q(\lambda)$ which, from the form of the full-conditionals above, results in
$$
\begin{aligned}
q^\star(\beta) &= \text{Normal}(\mu_{q(\beta)}, \Sigma_{q(\beta)}) \\
q^\star(\sigma^2) &= \text{Inverse-Gamma}(a_{q(\sigma^2)}, b_{q(\sigma^2)}) \\
q^\star(\lambda_i) &= \text{Inverse-Gamma}(a_{q(\lambda_i)}, b_{q(\lambda_i)}) \\
q^\star(\nu) &= \frac{\exp\left\{n\left[\frac{\nu}{2}\ln(\nu/2)-\ln\Gamma(\nu/2)\right]-(\nu/2)C_\nu\right\}}{\mathcal{F}(0,n,C_\nu,\nu_0,\nu_1)} \\
C_\nu &= \sum_{i=1}^n \mathbb E[\ln\lambda_i] + \mathbb E[\lambda_i^{-1}] \\
&= \sum_{i=1}^n \ln(b_{q(\lambda_i)}) - \psi\left(\frac{1}{2}(\mathbb \mu_{q(\nu)}+1)\right) + \frac{a_{q(\lambda_i)}}{b_{q(\lambda_i)}} \\
\mu_{q(\nu)} &= \frac{\mathcal{F}(1,n,C_\nu,\nu_0,\nu_1)}{\mathcal{F}(0,n,C_\nu,\nu_0,\nu_1)} \\
a_{q(\lambda_i)} &= \frac{\mu_{q(\nu)} + 1}{2}\\
b_{q(\lambda_i)} &= \frac{1}{2}\left[\mu_{q(\nu)} + \frac{a_{q(\sigma^2)}}{b_{q(\sigma^2)}}\left\{(y-X\mu_{q(\beta)})_i^2 + (X\Sigma_{q(\beta)}X^\top)_{ii}\right\}\right] \\
D_\lambda &= \text{diag}(a_{q(\lambda_i)}/b_{q(\lambda_i)}) \\
a_{q(\sigma^2)} &= a_0 + n/2\\
b_{q(\sigma^2)} &= b_0 + \frac{1}{2}\left[(y-X\mu_{q(\beta)})^\top D_\lambda(y-X\mu_{q(\beta)})+\text{tr}(\Sigma_{q(\beta)}X^\top D_\lambda X)\right]\\
\Sigma_{q(\beta)} &= \left(\frac{a_{q(\sigma^2)}}{b_{q(\sigma^2)}}X^\top D_\lambda X + \Sigma_0^{-1}\right)^{-1}\\
\mu_{q(\beta)} &= \Sigma_{q(\beta)}\left(\frac{a_{q(\sigma^2)}}{b_{q(\sigma^2)}}X^\top D_\lambda y + \Sigma_0^{-1}\mu_0\right).
\end{aligned}
$$

The ELBO is then
$$
\begin{aligned}
\mathcal{L}(q) &= \mathbb E[\ln p(y,\beta,\sigma^2,\lambda,\nu) - \ln q(\beta,\sigma^2,\lambda,\nu)] \\
&=
\end{aligned}
$$


# Examples



# References